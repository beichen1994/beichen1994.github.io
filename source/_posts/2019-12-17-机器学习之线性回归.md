---
title: 机器学习之线性回归
date: 2019-12-17 13:10:19
tags:
- machine learning
categories: 
- Computer
---


&ensp;&ensp;线性回归算法的思想也比较简单，可以用来解决二分类问题。

&ensp;&ensp;我们的目标是要找到一条界线$y=ax+b$,这条界线可以尽可能的区分两种属性。

&ensp;&ensp;假设在一个坐标系中有黑白两种类型的圆点，对于每一个$x^{i}$,都有一个真值$y^{i}$与之对应，但我们在寻找这条界线的过程中，对于每一个$x^{i}$,我们都有一个$y^{'i}$，我们需要勇敢的面对这其中的损失，即$y^{i}-y^{'i}$。

&ensp;&ensp;但这个损失有正数也有负数，不利于我们进行计算。所以，对于他们之间的损失，我们需要采用$(y^{i}-y^{'i})^2$进行统计。如果考虑所有样本，则损失函数可以表示为$\sum_{i=1}^{m}(y^{i}-y^{'i})^2$。

&ensp;&ensp;所以线性回归算法要解决的问题，就是要找到界线的系数$a$ $b$,使得$\sum_{i=1}^{m}(y^{i}-y^{'i})^2$尽可能小，即最小化误差的平方。

&ensp;&ensp;我们可以采用最小二乘法来解决这个问题，如果对这个推导不感兴趣可以直接跳过。
设
$$J(a,b)$$ 

$$=\sum_{i=1}^{m}(y^{i}-y^{'i})^2$$

$$=\sum_{i=1}^{m}(y^{i}-ax^{i}-b)^2$$

现在我们要使得

$$\frac{\partial J(a,b)}{\partial a}=0$$

$$\frac{\partial J(a,b)}{\partial b}=0$$

首先我们对b求导

$$\frac{\partial J(a,b)}{\partial b}$$

$$=\sum_{i=1}^{m}2(y^{i}-ax^{i}-b)(-1)$$

$$=0$$

即

$$\sum_{i=1}^{m}(y^{i}-ax^{i}-b)=0$$

$$\sum_{i=1}^{m}y^{i}-a\sum_{i=1}^{m}x^{i}-\sum_{i=1}^{m}b=0$$

$$\sum_{i=1}^{m}y^{i}-a\sum_{i=1}^{m}x^{i}-mb=0$$

$$\sum_{i=1}^{m}y^{i}-a\sum_{i=1}^{m}x^{i}=mb$$

最终得到

$$b=\overline{y} - a\overline{x}$$

接下来我们对a求导，对a的求导比较麻烦。

$$\frac{\partial J(a,b)}{\partial a}$$

$$=\sum_{i=1}^{m}2(y^{i}-ax^{i}-b)(-x^{i})$$

$$=0$$

即

$$\sum_{i=1}^{m}(y^{i}-ax^{i}-b)(x^{i})=0$$

因为

$$b=\overline{y}-a\overline{x}$$

所以

$$\sum_{i=1}^{m}(y^{i}-ax^{i}-\overline{y}+a\overline{x})(x^{i})=0$$

$$\sum_{i=1}^{m}(y^{i}x^{i}-a(x^{i})^2-x^{i}\overline{y}+a\overline{x}x^{i})=0$$

$$\sum_{i=1}^{m}(y^{i}x^{i}-x^{i}\overline{y})-\sum_{i=1}^{m}(a(x^{i})^2-a\overline{x}x^{i})=0$$

$$\sum_{i=1}^{m}(y^{i}x^{i}-x^{i}\overline{y})= a\sum_{i=1}^{m}((x^{i})^2-\overline{x}x^{i})$$

所以

$$a=\frac{\sum_{i=1}^{m}(y^{i}x^{i}-x^{i}\overline{y})}{a\sum_{i=1}^{m}((x^{i})^2-\overline{x}x^{i})}$$

因为
$$\sum_{i=1}^{m}x^{i}\overline{y}=\overline{y}\sum_{i=1}^{m}x^{i}=m\overline{y}*\overline{x}$$

$$=\sum_{i=1}^{m}\overline{y}*\overline{x}= \overline{x}\sum_{i=1}^{m}y^{i}$$

所以

$$a=\frac{\sum_{i=1}^{m}(x^{i}y^{i}-x^{i}\overline{y}-\overline{x}y^{i}+\overline{x}*\overline{y})}{\sum_{i=1}^{m}((x^{i})^2-\overline{x}x^{i}-\overline{x}x^{i}+(\overline{x})^2)}$$

$$=\frac{\sum_{i=1}^{m}(x^{i}-\overline{x})(y^{i}-\overline{y})}{\sum_{i=1}^{m}(x^{i}-\overline{x})^2}$$

至此，我们已经得到了参数a、b的计算公式。

$$a=\frac{\sum_{i=1}^{m}(x^{i}-\overline{x})(y^{i}-\overline{y})}{\sum_{i=1}^{m}(x^{i}-\overline{x})^2}$$

$$b=\overline{y}-a\overline{x}$$

&ensp;&ensp;接下来我们就可以根据数据集对参数a、b进行训练，得到我们想要的那条直线。

&ensp;&ensp;当然，我们也不用非得自己编写这样一个线性回归算法，也有现成的API供我们调用，如scikit-learn中就已经包装了线性回归算法，调用起来也非常方便。但如果我们可以自己去写出这样一个线性回归算法时，我们对该算法的理解就会变得非常深。

&ensp;&ensp;加油吧！



