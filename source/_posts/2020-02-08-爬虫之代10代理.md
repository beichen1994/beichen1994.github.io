---
title: 爬虫之代10代理
date: 2020-02-08 22:55:00
tags:
- 爬虫
categories: 
- Computer
---


# 基本原理
服务器检测的是某个IP单位时间的请求次数
代理即代理服务器(proxy server)
本机不是直接向web服务器发送请求，而是向代理服务器发送请求，再转发给服务器。并把服务器的响应转发给本机
# 作用
1.突破自身IP限制
2.访问一些单位或团体内部资源
3.提高访问速度，代理服务器会设置一个较大的硬盘缓冲区
4.隐藏真实IP
# 代理分类
## 根据协议区分
FTP代理服务器
HTTP.SSL/TLS.RTSP.Telnet.POP3/SMTP.SOCKET
## 根据匿名程度区分
高度匿名代理:数据包原封不动转发，服务器以为是普通客户端但其实为代理IP
普通匿名代理：数据包做改动，服务器端会发现代理IP，并会追查到真实IP
透明代理：数据包改动，且告诉服务器真实IP
间谍代理：用于记录用户传输的数据，进行研究，监控
# 常见代理设置
免费代理：可用代理不多，需要筛选，也可以维护一个代理池
付费代理
ADSL拨号：拨一次号换一次IP

# 代理池
```
from bs4 import BeautifulSoup
import requests
import json



headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'}


#从xicidaili首页获取一个代理ip
def get_ip_list(url, headers):
    #得到该页面的所有IP
    html = requests.get(url, headers=headers)
    html_text = html.text
    #得到网页响应
    soup = BeautifulSoup(html_text, 'lxml')
    #解析网页

    ips = soup.find_all('tr')
    ip_list = []

    for i in range(1, len(ips)):
        ip_info = ips[i]

        tds = ip_info.find_all('td')    #一个tr中的所有td

        #httptype=str.lower(tds[5].text)  # HTTP/TTPS-->http/https
        #ip_list.append(httptype+'://'+tds[1].text + ':' + tds[2].text) #构造一个链接

        ip_list.append(tds[1].text + ':' + tds[2].text) #得到ip:port

    return ip_list




#构建格式化的单个proxies
def get_proxy(ip_port):

    proxy_ip = 'http://' + ip_port
    proxy_ips = 'https://' + ip_port
    proxies = {'https': proxy_ips, 'http': proxy_ip}
    return proxies




def verify_proxies(proxies):
    http_url = "https://www.mzitu.com/"
    try:
        r=requests.get(http_url,proxies=proxies)
    except Exception as e:
        print('invalid \n',e)
        return False
    else:
        if r.status_code>=200 and r.status_code<=300:
            print("{0}valid\n ".format(proxies))
            return True
        else:
            print("{0}invalid\n".format(proxies))
            return False


if __name__ == '__main__':
    url = 'http://www.xicidaili.com/nn/'

    ip_list = get_ip_list(url, headers=headers)
    valid_proxies_list=[]

    for i in range(1,len(ip_list)):



        proxies = get_proxy(ip_list[i])

        verify = verify_proxies(proxies)
        if verify == True:
            valid_proxies_list.append(proxies)
            with open('valid_proxies_list.json','w',encoding='utf-8') as file:
                file.write(json.dumps(valid_proxies_list,indent=2,ensure_ascii=False))




```
