---
title: 机器学习之knn
date: 2019-12-17 09:22:38
tags:
- machine learning
categories: 
- Computer
---

## knn
&ensp;&ensp;knn(k-nearest-neighbor)，顾名思义，就是k个距离最近的邻居，算法思想非常简单，可以用于解决分类问题。俗话说人以类聚，物以群分，如果我的邻居都喜欢安宁静谧的生活环境，那很大程度上我也是这样的人，否则我干嘛要跟他们做邻居呢？

[![QIdhBd.md.jpg](https://s2.ax1x.com/2019/12/17/QIdhBd.md.jpg)](https://imgchr.com/i/QIdhBd)


&ensp;&ensp;在这张图片中，当k=3时，我们会发现右上角的点距离最近的3个点都是蓝色，那么右上角的点则属于蓝色。

## 距离怎么算？
&ensp;&ensp;在knn中，对于距离的定义各有不同，毕竟我们需要确定一种距离的计算方法，在高中我们都学习过欧拉距离公式，此外还有曼哈顿距离、明可夫斯基距离，当我们把这几个距离公式统一一下，就会发现他们的本质是一样的。


* 欧拉距离
$$\sqrt{(x_1^a-x_1^b)^2+(x_2^a-x_2^b)^2+\cdots+(x_n^a-x_n^b)^2}$$
$$\sqrt{\sum_{i=1}^{n}(x_i^a-x_i^b)^2}$$
$$(\sum_{i=1}^{n}\|x_{i}^a-x_{i}^b\|^2)^{\frac{1}{2}}$$
* 曼哈顿距离
$$\sum_{i=1}^{n}\|x_{i}^a-x_{i}^b\|$$
$$(\sum_{i=1}^{n}\|x_{i}^a-x_{i}^b\|^2)^{\frac{1}{1}}$$
* 明可夫斯基距离
$$(\sum_{i=1}^{n}\|x_{i}^a-x_{i}^b\|^p)^{\frac{1}{p}}$$


&ensp;&ensp;我们会发现，这三种距离公式的参数p是不同的，而这就是knn算法的一个超参数。

&ensp;&ensp;除了以上三种距离之外，还有很多的距离计算方法，如向量空间余弦相似度、调整余弦相似度、皮尔森相关系数、Jaccrard相似系数等。

## 数据归一化
&ensp;&ensp;数据一般由于量纲的不同，在计算数据样本之间的距离时，很容易被其中某一种变量所主导，所以我们需要对数据做归一化处理，让数据样本中的变量处于同一量纲之下。

for example

|  | 肿瘤大小 | 发现时间 | 
| --- | --- | --- |
| 样本1 | 1cm | 200天 | 
| 样本2 | 5cm | 10天 |


* 最值归一化(normalization)
即把所有数据全部映射到0~1之间
$$
x_{scale} = \frac{x-x_{min}}{x_{max}-x_{min}}
$$
* 均值方差归一化(standardization)
即把所有数据全部映射到均值为0，方差为1的分布中
$$
x_{scale} = \frac{x-x_{mean}}{s}
$$
其中，分子上的为均值，分母上的为方差
&ensp;&ensp;我们要注意的是，当对测试数据集进行归一化时，要使用训练数据集的均值与方差，即
$$\frac{x\_test-mean_{train}}{std_{train}}$$





