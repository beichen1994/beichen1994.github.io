---
title: 深度学习-RNN
date: 2020-03-23 10:14:10
tags:
- neural network and deep learning
categories: 
- Computer
---


## 传统的BP(back propagation)神经网络结构

* 分为输入层,隐含层,输出层三层.在输出层,比较预测值与实测值,得到误差,调整隐含层的连接权重,并动态增删神经元
* 具体的网络输出并不会影响下一次的网络输入信号,最终保留的是隐含层的结构和连接权重

## Recurrent Neural Network

文本是典型的**序列数据**,后续的文本内容和前导文本内容之间存在**语义关联**,传统的神经网络模型并不能匹配这一数据特征.

希望分类器能够记得上下文的内容并用于预测,特别是将上文内容用于预测.

RNN是包含循环的神经网络,允许原有输入信息的持久化,RNN是对应问嗯这汇总序列类型数据的最自然的神经网络架构

每个节点的计算,都是基于当前输入和原先输入内容的**记忆**(原先的输出)而来

$$
s_{t} = f(Ux_{t}+Ws_{t-1})
$$

$Ux_{t}$中$U$是新数据所占的百分比
$Ws_{t-1}$中$W$是原先输入的记忆所占的百分比
$s_{t}$是输入的节点

RNN存在的问题

* RNN的记忆力不够久!当相关信息和预测词位置之间的间隔小时,RNN的表现很好.但是需要联想前面很远的信息进行预测! The vanishing(消失的) gradient probelm for RNNs.

## Long Short Term网络

* 属于RNN的一种网络,可以学习长期依赖信息.
* 好记性不如烂笔头，LSTM默认数据中有很多长期信息，因此直接用一个小本子(cell)来记录信息
* 但是信息中也有很多无用的，因此模型会随机丢弃一部分信息，通过模型训练，不断保留有价值的信息，丢弃无用的信息

* cell状态：在图上水平贯穿运行。类似于传送带，信息在上面持续传输，中间会有各种门进行信息流的更新。

* 门：用于控制信息的增加和减少的阀门。输出一个0-1之间的值，1表示这一趟的信息全部输入信息流，0表示这一趟的信息全部忘记（信息流无变化）

* 忘记门：决定从cell状态中丢弃什么信息，它把上一次的状态$h_{t-1}$和这一次的输入$x_{t}$合并进行处理。即$$f_{t} = \sigma(W_{f}.(h_{t-1},x_{t})+b_{f})$$，$f_{t}$会在更新门中从原有的cell状态中减去。
![87SVQP.png](https://s1.ax1x.com/2020/03/23/87SVQP.png)

* 记忆门：哪些信息应该被记住，首先，用$sigmoid$决定什么信息需要被忘记。即$$i_{t} = \sigma(W_{i}.(h_{t-1},x_{t})+b_{i})$$，然后用$tanh$决定哪些新增信息需要被记住，即$$C_{t} = \sigma(W_{C}.(h_{t-1},x_{t})+b_{C})$$
![87SAzt.png](https://s1.ax1x.com/2020/03/23/87SAzt.png)

* 更新门：把老的cell状态更新为新的cell状态，XOR：去除$f_{t}$和$C_{t-1}$中相同的信息，比如，$f_{t}$中有1，$C_{t-1}$中有1，2，3，则更新门输出2，3. AND：增加信息。将忘记门中的信息与记忆门中的信息合并，即$$C_{t}=f_{t} XOR C_{t-1} AND i_{t} XOR C_{t}$$，得到当前单元的cell状态。
![87SneS.png](https://s1.ax1x.com/2020/03/23/87SneS.png)

* 输出门：由记忆即$C_{t}$来决定输出什么值。即$$o_{t} = \sigma(W_{o}(h_{t-1},x_{t})+b_{o})$$,$$h_{t}=o_{t}*tanh(C_{t})$$
![87SkRI.png](https://s1.ax1x.com/2020/03/23/87SkRI.png)


