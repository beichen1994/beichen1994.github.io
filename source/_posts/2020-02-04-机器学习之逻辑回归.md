---
title: 机器学习之逻辑回归
date: 2020-02-04 14:04:44
tags:
- machine learning
categories: 
- Computer
---

&ensp;&ensp;国家目前正全力打赢这场"疫情攻防战"，我老实的呆在家里学习，不给祖国添乱，便是对国家最大的贡献。趁着寒假这段时间，将逻辑回归的学习笔记整理如下！

### 逻辑回归模型

&ensp;&ensp;逻辑回归用以解决分类问题，但是回归怎么能够解决分类问题呢?原来是逻辑回归将样本的特征和样本发生的概率联系起来，而样本发生的概率是一个数，所以其是一个回归问题。

&ensp;&ensp;我们知道机器学习的本质是$y^{'}=f(x)$，根据给出的样本值得到预测值$y^{'}$。而在逻辑回归中，则是
$$p^{'}=f(x)$$
$$
y^{'}=\begin{cases} 1,p^{'}>=0.5\\0,p^{'}<0.5
\end{cases}
$$
根据给出的样本值得到一个概率，根据概率值确定预测值$y^{'}$。

&ensp;&ensp;因此逻辑回归既可以看作回归算法，也可以看作是分类算法，但通常作为分类算法用，只可以解决二分类问题。

&ensp;&ensp;在线性回归中的$y^{'}=f(x)$ 其实是
$$y^{'}=\theta^{T}.x_{b}$$
其值域范围为$(-\infty,+\infty)$。而逻辑回归中的值域范围为(-1,1)，因此，逻辑回归的$y^{'}=f(x)$ 其实是$$p^{'}=\sigma(\theta^{T}.x_{b})$$
其中$\sigma$是sigmoid函数，即$$\sigma(t)=\frac{1}{1+e^{-t}}$$相当于神经网络中的激活函数。
[![1BrQjP.md.png](https://s2.ax1x.com/2020/02/04/1BrQjP.md.png)](https://imgchr.com/i/1BrQjP)
因此，逻辑回归的模型为$$p^{'}=\sigma(\theta^{T}.x_{b})=\frac{1}{1+e^{-\theta^{T}.x_{b}}}$$
$y^{'}=\begin{cases}1,p^{'}>=0.5\\0,p^{'}<0.5 \end{cases}$

&ensp;&ensp;那么在逻辑回归中，对于给定的样本数据集$X,y(真实值)$，我们该如何找到参数$\theta$，使得最大程度的获得样本数据集X对应的分类输出y？即使得逻辑回归的损失函数值最小呢？

### 逻辑回归的损失函数

&ensp;&ensp;根据逻辑回归的模型，我们可以得到
$$cost = \begin{cases} 如果y=1,p^{'}越小，cost越大\\如果y=0,p^{'}越大，cost越大 \end{cases}$$
因为如果真实值y=1,概率值越小，预测值$y^{'}$越接近于0，则损失值cost越大。反之，如果真实值y=0,概率值越大，预测值$y^{'}$越接近于1，则损失值cost越大。

&ensp;&ensp;而对于此类损失函数，我们可以想到log函数正好符合这样的特性。即
$$cost = \begin{cases}-log(p^{'})&if&y=1 \\-log(1-p^{'})&if&y=0 \end{cases}$$
他们的函数图像如下所示：
[![1Br3B8.md.png](https://s2.ax1x.com/2020/02/04/1Br3B8.md.png)](https://imgchr.com/i/1Br3B8)
综上，逻辑回归的损失函数为
$$cost = -ylog(p^{'}-(1-y)log(1-p^{'}))$$

即
$$J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}y^{i}log(p^{'i})+(1-y^{i})log(1-p^{'i}))$$
其中
$$p^{'i}=\sigma(X_{b}^{i}\theta)=\frac{1}{1+e^{-X_{b}^{i}\theta}}$$

&ensp;&ensp;对于逻辑回归的损失函数，我们可以通过梯度下降法求解它的最小值，不过我们需要利用高中所学的求导公式进行求导(比较麻烦，就先省略了)，最后他的导数为
$$\nabla J(\theta) = \frac{1}{m}X_{b}^{T}(\sigma(X_{b}\theta)-y)$$

## 未完待续