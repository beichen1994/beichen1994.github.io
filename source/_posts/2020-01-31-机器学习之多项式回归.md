---
title: 机器学习之多项式回归
date: 2020-01-31 10:53:40
tags:
- machine learning
categories: 
- Computer
---

## 多项式回归

&ensp;&ensp;寒假期间，我来将多项式回归算法的笔记整理一下。

&ensp;&ensp;多项式回归算法主要用来处理非线性数据。如果我们想要拟合非线性数据，如$y=ax^2+bx+c$,我们可以将$x^2$作为一个特征，$x$作为另一个特征，即可当作线性回归进行处理，相当于为样本多添加了一些特征(原来样本的多项式项)

&ensp;&ensp;在sklearn中，如果原样本有$x_{1},x_{2}$两个特征，调用多项式回归算法时，PolynomiaFeature(degree=2)(**多项式特征**)会产生如下特征:1,$x_{1}^{1}$,$x_{2}^{1}$,$x_{1}^{2}$,$x_{2}^{2}$。如果degree=3，则产生如下特征:1,$x_{1}^{1}$,$x_{2}^{1}$,$x_{1}^{2}$,$x_{2}^{2}$,$x_{1}*x_{2}$,$x_{1}^{3}$,$x_{2}^{3}$,$x_{1}^{2}*x_{1}$,$x_{1}*x_{2}^{2}$。

&ensp;&ensp;degree越高，拟合的效果越好，产生overfitting过拟合，算法所训练的模型过多的表达了数据间的噪音关系。如果非线性数据使用线性回归算法，则会造成underfitting欠拟合，算法所训练的模型不能完整的表述数据间的关系。

&ensp;&ensp;而随着模型复杂度的提高，训练数据集上模型准确率会越来越高，测试数据集上，模型准确率会先高后低，出现过拟合。为了防止模型出现过拟合或者欠拟合，我们需要将数据分为训练数据集和测试数据集，训练数据集用以训练模型，测试数据集用以评估模型的泛化能力。

***
&ensp;&ensp;随着训练样本的增多，模型的表现能力(误差的变化、稳定值的变化)有怎样的变化？变化的曲线则为learning-curve学习曲线。
***
&ensp;&ensp;但如果模型对特定的测试数据集出现过拟合该怎么办呢？因此，我们可以将数据分为训练数据集、验证数据集、测试数据集。训练数据集用以训练数据，验证数据集用以评判模型，调整超参数，测试数据集作为衡量最终模型性能的数据集。

&ensp;&ensp;此时，我们可以使用cross-valiadition交叉验证。
[![18tLKU.md.png](https://s2.ax1x.com/2020/02/01/18tLKU.md.png)](https://imgchr.com/i/18tLKU)
[![18tbvT.md.png](https://s2.ax1x.com/2020/02/01/18tbvT.md.png)](https://imgchr.com/i/18tbvT)
[![18tH2V.md.png](https://s2.ax1x.com/2020/02/01/18tH2V.md.png)](https://imgchr.com/i/18tH2V)


***
&ensp;&ensp;针对模型出现的误差，我们肯定是希望误差越小越好。但这需要我们采用一些小技巧和付出一些努力！一般认为模型误差=偏差(Bias)+方差(Variance)+不可避免地误差。偏差和方差如下图所示。
[![18tXb4.md.png](https://s2.ax1x.com/2020/02/01/18tXb4.md.png)](https://imgchr.com/i/18tXb4)

&ensp;&ensp;其中预测的问题本身是靶子中心，拟合的模型则是打出的枪。左下角是高偏差(很明显模型的结果都偏于靶心),右上角是高方差(很明显模型的结果都分散于靶心周围)。导致偏差的主要原因是对于问题本身的假设不正确，如非线性数据使用线性回归，造成underfitting。导致方差的主要原因是模型没有学习到问题的本质，而学习到很多的噪音，如模型太复杂，使用高阶多项式回归，造成overfitting。

&ensp;&ensp;有一些算法天生是高方差算法，如KNN，其高度依赖于样本数据，不依赖于参数，有一些算法天生是高偏差算法，如线性回归。

&ensp;&ensp;非参数学习的算法通常都是高方差的算法，因为不对数据假设,只能根据现有的训练数据进行预测，特别依赖现有的训练数据的准确度。参数学习通常都是高偏差算法，因为，将整个问题归结为一个数学模型，求出该模型相应的参数即可，对数据具有极强的假设。


&ensp;&ensp;大多数算法具有相应的参数，可以调整偏差和方差。如KNN中的k，k越小，模型越复杂，偏差越小。k越大，模型越简单，直到等于样本总数时，knn样本中谁最多则预测谁，则偏差最大，方差最小。再比如使用多项式回归，degree越大，模型越复杂，偏差越小，方差越大，degree越小，接近1，模型越简单，偏差越大。降低偏差，会提高方差，降低方差，则会提高偏差。而机器学习的主要挑战，来自于方差。

&ensp;&ensp;为了解决高方差，我们有以下手段：
1、 降低模型复杂度
2、 减少数据维度，降噪（减少对噪声的学习），如PCA
3、 增加样本数
4、 使用验证集
5、 模型正则化

&ensp;&ensp;这里着重说一下模型正则化Regularization，正则化是要约束参数的大小。那到底会怎么约束呢，且看下文！

&ensp;&ensp;我们知道线性回归的目标是
$$\sum_{i=1}^{m}(y^{i}-\theta_{0}-\theta_{1}x_{1}^{i}-\theta_{2}x_{2}^{i})-...\theta_{n}x_{n}^{i}$$
尽可能小，对于损失函数，则
$$J(\theta)=MSE(y,y^{'};\theta)$$
尽可能小。加入模型正则化后，则
$$J(\theta)=MSE(y,y^{'};\theta)+\alpha*\frac{1}{2}\sum_{i=1}^{m}\theta_{i}^{2}$$
尽可能小。此时加入的模型正则化为Ridge Regression岭回归。

&ensp;&ensp;为了让$J(\theta)$尽可能小，则必须考虑让每一个$\theta$尽可能小。甚至会让某些$\theta$值为0。正好减少数据特征，减少模型的高方差。

&ensp;&ensp;除了Ridge Regression外，还有LASSO Regression(least Absolute Shrinkage and **Selection** Operator Regression)，加入LASSO Regression后，则使得
$$J(\theta)=MSE(y,y^{'};\theta)+\alpha*\sum_{i=1}^{m}\|\theta_{i}\|$$
尽可能小。LASSO趋向于使得一部分$\theta$值变为0，所以可作为特征选择用。

&ensp;&ensp;我们注意到，$\alpha$是一个新的超参数，为0时，则相当于没有正则化，当正无穷时，则MSE的比重很小，只有当每一个$\theta$都接近于0时，则会让$J(\theta)$尽可能小。

***
&ensp;&ensp;现在我们将学习到的距离公式，误差函数，正则化整理一下，会发现他们之间是有规律和联系的。

|  |  |  |
| --- | --- | --- |
| Ridge(L2范数、L2正则化) | MSE | 欧拉距离 |
| $\|x\|_{2} = \frac{1}{2}\sum_{i=1}^{m}\theta_{i}^{2}$ | $\frac{1}{n}\sum_{i=1}^{m}(y_{i}-y_{i}^{'})^{2}$ | $(\sum_{i=1}^{m}(x_{i}^{1}-x_{i}^{2})^{2})^{\frac{1}{2}}$ |    |
| LASSO(L1范数、L1正则化) | MAE | 曼哈顿距离 |
| $\|x\|_{1} = \sum_{i=1}^{m}\|\theta_{i}\|$ | $\frac{1}{n}\sum_{i=1}^{m}\|y_{i}-y_{i}^{'}\|$ | $\sum_{i=1}^{m}\|x_{i}^{1}-x_{i}^{2}\|$ |      |
|范数|   |明可夫斯基距离|   |
| $\|x\|_{p} = (\sum_{i=1}^{m}\|x_{i}\|^{p})^{\frac{1}{p}}$ |   |($\sum_{i=1}^{m}(x_{i}^{1}-x_{i}^{2})^{p})^{\frac{1}{p}}$   |  

&ensp;&ensp;总之，各种回归或者正则，都是给MSE添加正则项，形成新的损失函数。最后，还有一种正则化，名之弹性网，则结合两种方式，并引入一个新的超参数r，来表示两种正则项的比例。
$$J(\theta) = MSE(y,y^{'};\theta)+r*\alpha\sum_{i=1}^{m}\|\theta_{i}\| + (1-r) \frac{1}{2}\alpha\sum_{i=1}^{m}(\theta_{i})^{2}$$
