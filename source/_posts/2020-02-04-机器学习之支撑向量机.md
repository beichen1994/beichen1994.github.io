---
title: 机器学习之支撑向量机
date: 2020-02-04 21:45:45
tags:
- machine learning
categories: 
- Computer
---

&ensp;&ensp;今天接着整理支撑向量机的学习笔记。
&ensp;&ensp;支撑向量机，简称SVM(Support Vector Machine),可以用来解决分类问题，同样也可以解决回归问题。特别是在分类问题中，如果各个分类样本的决策边界不唯一，可以用于解决不适定问题。
[![1DWLL9.md.png](https://s2.ax1x.com/2020/02/04/1DWLL9.md.png)](https://imgchr.com/i/1DWLL9)

&ensp;&ensp;我们在做分类时，需要找到一个决策边界，不仅要将现有的训练数据的各个类别进行划分，还要有很好的泛化能力，对未来的测试数据也要能够很好的划分，要离我们的各个分类样本尽可能的远。即距离两个类别的最近的样本最远。这些最近的样本点称为支撑向量。对于这样的线性可分的问题，称为Hard Margin SVM。而线性不可分的问题，则称为Soft Margin SVM。如下图所示
[![1DW8PK.md.png](https://s2.ax1x.com/2020/02/04/1DW8PK.md.png)](https://imgchr.com/i/1DW8PK)
因为$$margin = 2d$$,所以，SVM就是要最大化margin，即最大化d。

&ensp;&ensp;我们在高中学过点$(x,y)$到直线$Ax+By+C=0$的距离公式为
$$\frac{|Ax+By+c|}{\sqrt{A^{2}+B^{2}}}$$。如果我们拓展到n维空间，直线方程为$$\theta^{T}*x_{b}=0$$,其中$\theta_{0}x_{0}$为截距b，则换一种形式为$$w^{T}x+b=0$$

&ensp;&ensp;假设决策边界$w^{T}x+b=0$,则点$(x,y)$到决策边界的距离为
$$\frac{|W^{T}x+b|}{\|w\|}$$
其中$\|w\|=\sqrt{w_{1}^{2}+w_{2}^{2}+...+w_{n}^{2}}$

&ensp;&ensp;支撑向量到决策边界的距离为d，其余样本点到决策边界的距离均大于d，所以
$$\begin{cases} \frac{|W^{T}x^{i}+b|}{\|w\|}>=d&\forall y^{i}=1\\ \frac{|W^{T}x^{i}+b|}{\|w\|}<=-d&\forall y^{i}=-1\end{cases} $$
即
$$\begin{cases} \frac{|W^{T}x^{i}+b|}{\|w\|d}>=1&\forall y^{i}=1\\ \frac{|W^{T}x^{i}+b|}{\|w\|d}<=-1&\forall y^{i}=-1\end{cases} $$
即
$$\begin{cases} |W_{d}^{T}x^{i}+b_{d}>=1&\forall y^{i}=1\\ W_{d}^{T}x^{i}+b_{d}<=-1&\forall y^{i}=-1\end{cases} $$

决策边界与支撑边界的变化如下图所示
![1DcY0x.png](https://s2.ax1x.com/2020/02/04/1DcY0x.png)
![1Dc09e.png](https://s2.ax1x.com/2020/02/04/1Dc09e.png)
![1DcatO.png](https://s2.ax1x.com/2020/02/04/1DcatO.png)

$$\therefore y^{i}(w^{T}x^{i}+b)>=1$$

&ensp;&ensp;而对于任意的支撑向量x，我们要最大化margin，最大化d，
$$\therefore max \frac{w^{T}x+b}{\|w\|}$$，
$$\therefore max \frac{1}{\|w\|}$$，
$$\therefore min \|w\|$$
$$\therefore min \frac{1}{2}w^{2}$$

&ensp;&ensp;最终我们得到，在$$y^{i}(w^{T}x^{i}+b)>=1$$的条件下，我们要$$min \frac{1}{2}w^{2}$$

&ensp;&ensp;我们来看下Soft Margin 和SVM的正则化。我们需要我们的SVM能够有一定的容错能力，因此支撑边界为$w^{T}x+b=1-\delta$，但也要保证容错能力尽可能小，所以我们要使得$$min\frac{1}{2}(\|w\|)^{2}+\sum_{i=1}^{m}\delta_{i}$$，$$\delta_{i}>=0$$，这样也可以确保$\delta$足够小。
如下图所示。
[![1DcB1H.md.png](https://s2.ax1x.com/2020/02/04/1DcB1H.md.png)](https://imgchr.com/i/1DcB1H)

&ensp;&ensp;对于Soft Margin SVM，我们可以使用前面所学习过的正则化，来最小化目标函数。如
|L1正则  |L2正则  |
|---|---|
| $min\frac{1}{2}(\|w\|)^{2}+C\sum_{i=1}^{m}\delta_{i}$ | $min\frac{1}{2}(\|w\|)^{2}+C\sum_{i=1}^{m}\delta_{i}^{2}$ |
其中都要保证$y^{i}(w^{T}x^{i}+b)>=1-\delta_{i}$，且$\delta_{i}>=0$

&ensp;&ensp;实际使用SVM，和KNN一样，要做数据标准化处理！因为涉及计算距离，如果数据点在不同的维度上的量纲不同，则在计算距离时造成一定的问题。
